# -*- coding: utf-8 -*-
"""DisasterPredictor_keras.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13dSF-T-gGF-n-sKVpcg-i4kU36pMOc-S

<a href="https://colab.research.google.com/github/bernardo-martins/DisasterPredictor/blob/main/DisasterPredictor_keras.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>

Imports and other installations
"""

import csv
import pandas as pd
import numpy as np
import tensorflow as tf

import nltk
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from gensim.parsing.preprocessing import remove_stopwords
from transformers import RobertaTokenizerFast
import spacy
import string
nlp = spacy.load('en_core_web_sm')

!pip install nltk
!pip install spacy
!pip install gensim
!python -m spacy download en_core_web_sm
!pip install transformers
!pip3 install tensorflow==1.12

!unzip /usr/share/nltk_data/corpora/wordnet.zip -d /usr/share/nltk_data/corpora/

NUM_BATCHES = 16
LSTM_UNITS = 128
EMBEDDING_DIM = 100
MAX_LENGTH = 64

import zipfile
import os

def unzip_and_load_data(zip_file_path, extract_to='.'):
    """
    Unzips a file and returns a list of extracted file paths.

    :param zip_file_path: The path to the zip file.
    :param extract_to: The directory where the files will be extracted.
    :return: A list of paths to the extracted files.
    """
    extracted_files = []

    # Check if the zip file exists
    if not os.path.exists(zip_file_path):
        raise FileNotFoundError(f"The zip file {zip_file_path} does not exist.")

    # Unzip the file
    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
        zip_ref.extractall(extract_to)
        extracted_files = zip_ref.namelist()

    # Return the list of extracted file paths
    extracted_file_paths = [os.path.join(extract_to, file) for file in extracted_files]

    return extracted_file_paths

def load_csv(file_path):
    """
    Loads a CSV file into a pandas DataFrame.

    :param file_path: The path to the CSV file.
    :return: A pandas DataFrame containing the CSV data.
    """
    return pd.read_csv(file_path)

def train_val_datasets(dataframe, labels_column):
    TRAINING_SPLIT = 0.9
    BATCH_SIZE = 128
    BUFFER_SIZE = 10000

    # Split the dataframe into features and labels
    features = dataframe.drop(labels_column, axis=1)
    labels = dataframe[labels_column]

    # Convert the DataFrame to a TensorFlow dataset
    dataset = tf.data.Dataset.from_tensor_slices((features.values, labels.values))

    # Shuffle the dataset
    dataset = dataset.shuffle(BUFFER_SIZE)

    total_size = len(dataframe)  # Use DataFrame length instead of dataset cardinality
    train_size = int(TRAINING_SPLIT * total_size)

    # Split into training and validation sets
    train_dataset = dataset.take(train_size)
    validation_dataset = dataset.skip(train_size)

    # Batch the datasets
    train_dataset = train_dataset.batch(BATCH_SIZE)
    validation_dataset = validation_dataset.batch(BATCH_SIZE)

    return train_dataset, validation_dataset

zip_file = 'nlp-getting-started.zip'  # Specify the zip file path
extract_dir = './extracted_data'  # Specify where to extract the files

extracted_files = unzip_and_load_data(zip_file, extract_dir)
for file in extracted_files:
    if file.endswith('.csv'):
        df = load_csv(file)

# Extract only the necessary data
df = df[['text', 'target']]

# Create the train and validation datasets
train_dataset, validation_dataset = train_val_datasets(df, 'target')

print(f"There are {len(train_dataset)} batches for a total of {NUM_BATCHES*len(train_dataset)} elements for training.\n")
print(f"There are {len(validation_dataset)} batches for a total of {NUM_BATCHES*len(validation_dataset)} elements for validation.\n")

"""Stop-words"""

def standardize(sentence):
  sentence = sentence.lower()

  sentence = remove_stopwords(sentence)

  return sentence

"""Vectorization"""

import tensorflow as tf

def fit_vectorizer(sentences):
    """
    Instantiates the TextVectorization layer and adapts it to the sentences.

    Args:
        sentences (list[str]): lower-cased sentences without stopwords

    Returns:
        tf.keras.layers.TextVectorization: an instance of the TextVectorization layer adapted to the texts.
    """

    tf.keras.utils.set_random_seed(65)  # Set random seed for reproducibility

    # Define the object
    vectorizer = tf.keras.layers.TextVectorization(
        standardize="lower_and_strip_punctuation",  # Use the defined function here
        ragged=True,  # Allow for variable-length sequences
        max_tokens=10000,  # Limit the vocabulary size
    )

    # Adapt it to the corpus
    vectorizer.adapt(sentences)

    return vectorizer

sentences = df['text'].astype(str).values
vectorizer = fit_vectorizer(sentences)

# Apply vectorization to train and val datasets
train_dataset_vectorized = train_dataset.map(lambda x,y: (vectorizer(x), y))
validation_dataset_vectorized = validation_dataset.map(lambda x,y: (vectorizer(x), y))

from tensorflow.keras.preprocessing.sequence import pad_sequences

train_sequences_padded = []
train_labels = []
validation_sequences_padded = []
validation_labels = []
dim = 100

for features, label in train_dataset_vectorized:
    # Convert features (which are tensors) to lists
    sequence = pad_sequences(features.numpy(), maxlen=dim, padding='post', dtype='float32')
    train_sequences_padded.append(sequence)  # Append the numpy array
    train_labels.append(label.numpy())
    assert sequence.shape[1] == dim

train_sequences_padded = np.array(train_sequences_padded[:-1])
train_labels = np.array(train_labels[:-1])

for features, label in validation_dataset_vectorized:
    # Convert features (which are tensors) to lists
    sequence = pad_sequences(features.numpy(), maxlen=dim, padding='post', dtype='float32')
    validation_sequences_padded.append(sequence)  # Append the numpy array
    validation_labels.append(label.numpy())
    assert sequence.shape[1] == dim

validation_sequences_padded = np.array(validation_sequences_padded[:-1])
validation_labels = np.array(validation_labels[:-1])

print(train_sequences_padded.shape)
print(train_labels.shape)
print("-----")
print(validation_sequences_padded.shape)
print(validation_labels.shape)

"""Using pre-defined Embeddings"""

# Define path to file containing the embeddings
glove_file = 'glove.6B.100d.txt'
glove_embeddings = {}

with open(glove_file) as f:
    for line in f:
        values = line.split()
        word = values[0]
        coefs = np.asarray(values[1:], dtype='float32')
        glove_embeddings[word] = coefs
word_index = {x:i for i,x in enumerate(vectorizer.get_vocabulary())}
vocab_size = vectorizer.vocabulary_size()

# Initialize an empty numpy array with the appropriate size
embeddings_matrix = np.zeros((vocab_size, EMBEDDING_DIM))

# Iterate all of the words in the vocabulary and if the vector representation for
# each word exists within GloVe's representations, save it in the embeddings_matrix array
for word, i in word_index.items():
    embedding_vector = glove_embeddings.get(word)
    if embedding_vector is not None and not embedding_vector.shape[0] != EMBEDDING_DIM:
        embeddings_matrix[i] = embedding_vector

"""Create Model"""

def create_lstm_model1(vocab_size, pretrained_embeddings):
    """
    Creates a binary sentiment classifier model using LSTM.

    Args:
        vocab_size (int): Number of words in the vocabulary.
        pretrained_embeddings (np.ndarray): Array containing pre-trained embeddings.

    Returns:
        (tf.keras.Model): the sentiment classifier model
    """

    model = tf.keras.Sequential([
        # Change input shape to (20,) if you're passing sequences of length 20
        tf.keras.Input(shape=(128, 100)),  # Input layer with sequences of length 20
        tf.keras.layers.Embedding(
            input_dim=vocab_size,
            output_dim=pretrained_embeddings.shape[1],
            weights=[pretrained_embeddings],
            trainable=False
        ),
        # LSTM Layer for sequence modeling
        #tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(pretrained_embeddings.shape[1], return_sequences=False)),
        tf.keras.layers.Reshape((128, 10000), input_shape=pretrained_embeddings.shape[1]),
        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),  # You can adjust the number of units as needed
        tf.keras.layers.Dropout(0.5),
        tf.keras.layers.Dense(32, activation='relu'),
        tf.keras.layers.Dense(1, activation='sigmoid')
    ])

    # Compile the model
    model.compile(
        loss='binary_crossentropy',
        optimizer='adam',
        metrics=['accuracy']
    )

    return model

def create_lstm_model():
    """
    Creates a binary sentiment classifier model using LSTM without the embedding layer,
    assuming input data is already pre-embedded.

    Returns:
        (tf.keras.Model): the sentiment classifier model
    """

    model = tf.keras.Sequential([
        # Input layer for sequences of embedded vectors (shape: 128 timesteps, 100-dimensional embedding)
        tf.keras.Input(shape=(None,)),

        # Dropout for regularization
        tf.keras.layers.Dropout(0.5),

        # Fully connected layer
        tf.keras.layers.Dense(64, activation='relu'),

        # Output layer for binary classification
        tf.keras.layers.Dense((1), activation='sigmoid')
    ])

    # Compile the model
    model.compile(
        loss='binary_crossentropy',
        optimizer='adam',
        metrics=['accuracy']
    )

    return model



def create_lstm_model2(vocab_size, pretrained_embeddings):
    model = tf.keras.Sequential([
        tf.keras.Input(shape=(128,100)),# Input layer with dynamic length
        tf.keras.layers.Dense(10000, activation='relu'),  # Fully connected layer tf.keras.layers.Reshape((128, 10000), input_shape=(10000,)
        tf.keras.layers.Embedding(
            input_dim=10000,           # Size of the vocabulary
            output_dim=100,  # Dimensionality of the embeddings
            weights=[pretrained_embeddings],  # Load pre-trained embeddings
            trainable=False                   # Do not train the embeddings
        ),
        tf.keras.layers.Reshape((128, 100*100*100)),
        tf.keras.layers.Conv1D(filters=64, kernel_size=3, padding='same', activation='relu'),  # Convolutional layer
        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),  # You can adjust the number of units as needed
        #tf.keras.layers.GlobalMaxPooling2D(),   # Max pooling layer
        #tf.keras.layers.Reshape((128, 10000), input_shape=(10000,)),
        tf.keras.layers.Dropout(0.5),           # Dropout layer to prevent overfitting
        tf.keras.layers.Dense(64, activation='relu'),  # Fully connected layer
        tf.keras.layers.Dense(1, activation='sigmoid')  # Output layer for binary classification
    ])

    # Compile the model
    model.compile(
        loss='binary_crossentropy',
        optimizer='adam',
        metrics=['accuracy']
    )

    return model

x = np.random.rand(2, 4, 5, 3)

print("vocab_size: ",vocab_size)
print("embeddings_matrix: ",embeddings_matrix.shape)
print("train_sequences_padded: ",train_sequences_padded.shape)

print("embeddings_matrix: ",embeddings_matrix.shape[1])


model = create_lstm_model2(vocab_size, embeddings_matrix)
model.summary()

train_dataset = tf.data.Dataset.from_tensor_slices((train_sequences_padded, train_labels))
validation_dataset = tf.data.Dataset.from_tensor_slices((validation_sequences_padded, validation_labels))

TRAIN_BATCH_SIZE = 32
VALIDATION_BATCH_SIZE = 32

train_dataset = train_dataset.batch(TRAIN_BATCH_SIZE, drop_remainder=True)
validation_dataset = validation_dataset.batch(TRAIN_BATCH_SIZE, drop_remainder=True)

example_batch = train_dataset.take(1)
#print(example_batch.cardinality())
model.evaluate(example_batch, verbose=False)

tf.config.run_functions_eagerly(True)

history = model.fit(
	x= train_dataset,
	epochs=20,
	validation_data=validation_dataset,
	verbose=1
	)

import matplotlib.pyplot as plt

# Get training accuracies and losses
acc = history.history['accuracy']
loss = history.history['loss']

# Get number of epochs
epochs = range(len(acc))

# Print accuracy and loss values
print("Accuracy:", acc)
print("Loss:", loss)

# Plot the performance
fig, ax = plt.subplots(1, 2, figsize=(10, 5))
fig.suptitle('Training performance')

# Plot Accuracy
ax[0].plot(epochs, acc, 'b', label='Training accuracy')
ax[0].set_title('Accuracy')
ax[0].set_xlabel('Epochs')
ax[0].set_ylabel('Accuracy')
ax[0].legend()

# Plot Loss
ax[1].plot(epochs, loss, 'r', label='Training loss')
ax[1].set_title('Loss')
ax[1].set_xlabel('Epochs')
ax[1].set_ylabel('Loss')
ax[1].legend()

plt.tight_layout()
plt.show()